# itk-version: 3.2.0
version: "3.8"

networks:
  frontend:
    external: true
  app:
    driver: bridge
    internal: false

services:
  nginx:
    image: nginxinc/nginx-unprivileged:alpine
    networks:
      - app
      - frontend
    depends_on:
      - vectorstore
    ports:
      - '8080'
    volumes:
      - ./.docker/templates:/etc/nginx/templates:ro
      - ./local_ui.html:/app/index.html
    environment:
      NGINX_WEB_ROOT: /app
      NGINX_PORT: 8080
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=frontend"
      - "traefik.http.routers.${COMPOSE_PROJECT_NAME}.rule=Host(`${COMPOSE_DOMAIN}`)"
#      HTTPS config - uncomment to enable redirect from :80 to :443
#      - "traefik.http.routers.${COMPOSE_PROJECT_NAME}.middlewares=redirect-to-https"
#      - "traefik.http.middlewares.redirect-to-https.redirectscheme.scheme=https"

  vectorstore:
    build: 
      context: .
      dockerfile: ./Dockerfile
    networks:
      - app
    ports:
      - "8000"
    depends_on:
      - model
    environment:
      - VDB_PATH=/model
      - MODEL_NAME=mistral
      # - MODEL_NAME='mistralai/Mistral-7B-Instruct-v0.2'
      - GEN_MODEL_URL=http://model:11434/api/chat
      - GEN_MODE=ollama,chat
    volumes:
      - ./vectorstores/sentence-transformers/all-MiniLM-l6-v2/:/model
      - ./src:/app
      - ./logs:/logs

  model:
    image: ollama/ollama
    networks:
      - app
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "11434"
    volumes:
      - ./.docker/data/ollama:/root/.ollama

  # model:
  #   image: vllm/vllm-openai
  #   networks:
  #     - app
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"
  #   ports:
  #     - "8000"
  #   volumes:
  #     - ./.docker/data/vllm:/root/.cache/huggingface
